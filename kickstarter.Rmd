---
title: "Kickstarter"
author: "Blake Brown and Jacob Fullerton"
date: "12/3/2018"
output: html_document
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r Library Packages, message=FALSE}
library(tidyverse)
library(readr)
library(caret)
library(keras)
library(vtreat)
library(randomForest)
library(gmodels)
library(adabag)
library(xgboost)
library(Matrix)
```


```{r Import data}

data <- read.csv("data.csv")

data$state <- ifelse((data$state == 'canceled')|(data$state=='suspended')|(data$state=='failed'), 'failed', 
                     ifelse((data$state == 'live')|(data$state == 'successful'), 'successful', NA))
data<- na.omit(data)

# data <- data %>% mutate(state=ifelse(state == 'canceled', 'failed', state))
# data <- data %>% mutate(state=ifelse(state == 'live', 'successful', state))
# data <- data %>% mutate(state=ifelse(state == 'suspended', 'failed', state))
# data <- data %>% filter(state='undefined')

data$state <- as.factor(data$state)
data$category <- as.factor(data$category)
data$main_category <- as.factor(data$main_category)
data$currency <- as.factor(data$currency)
data$country <- as.factor(data$country)
summary(data)

data$timeActive <- difftime(data$deadline , data$launched , units = c("days"))
data$timeActive <- as.numeric(data$timeActive)

data$binnedBackers <- ifelse(data$backers <= 20, 1,
                             ifelse((data$backers > 20) & (data$backers <= 100),2, 
                                    ifelse((data$backers > 100) & (data$backers <= 1000),3,4)))

data <- data %>% filter(timeActive < 365) ## found observations with really long active times
data$timeActiveBinned <- ifelse(data$timeActive <= 7, 1,
                             ifelse((data$timeActive > 7) & (data$timeActive <= 31),2, 
                                    ifelse((data$timeActive > 31) & (data$timeActive <= 62),3,4)))

str(data)
```



```{r}
outcome <- 'state'
target <- 2
tooDetailed <- c("name")
vars <- setdiff(colnames(data), c(tooDetailed))
dTrain <- data


```





```{r}
set.seed(4623762)
tplan <- vtreat::designTreatmentsZ(dTrain, vars, 
                                   minFraction= 0,
                                   verbose=FALSE)
# restrict to common varaibles types
# see vignette('vtreatVariableTypes', package = 'vtreat') for details
sf <- tplan$scoreFrame
newvars <- sf$varName[sf$code %in% c("lev", "clean", "isBAD")] 
trainVtreat <- as.matrix(vtreat::prepare(tplan, dTrain, 
                                         varRestriction = newvars))
print(dim(trainVtreat))

```


```{r}
output <- as.data.frame(trainVtreat)
dataNew <- output
#print(colnames(dataNew))

```

```{r}
set.seed(124)
ind <- createDataPartition(dataNew$state_lev_x_successful, p=0.8, list = FALSE)
dataNew <- dataNew[-199]
#print(colnames(dataNew))
x_train <- dataNew[ind,-199]
x_train_label <- dataNew[ind,199]
x_train_label <- as.data.frame(x_train_label)
x_test <- dataNew[-ind,-199]
x_test <- as.data.frame(x_test)
x_test_label <- dataNew[-ind,199]
x_test_label <- as.data.frame(x_test_label)

#x_train_label <- as.factor(x_train_label)
#x_test_label <- as.factor(x_test_label)

```




```{r}

# Normalize training data
x_train <- scale(x_train) 

# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(x_train, "scaled:center") 
col_stddevs_train <- attr(x_train, "scaled:scale")
x_test <- scale(x_test)

```

Now we define our model (using keras syntax):

```{r}
network <- keras_model_sequential() %>% layer_dense(units = 200, activation = "relu", input_shape = dim(x_train)[2]) %>% layer_dense(units = 100, activation = "relu") %>% layer_dense(units = 8, activation = "relu") %>%layer_dense(units = 8, activation = "relu") %>%layer_dense(units = 8, activation = "relu") %>%layer_dense(units = 1, activation = "sigmoid")

```

Now we need to compile our model, determining our optimizer (e.g., gradient descent), our loss function (e.g., sum of squared error) and the metric by which to measure our performance (e.g., accuracy)

```{r}
network %>% compile(loss = "binary_crossentropy", optimizer = optimizer_adam(lr = 0.0005), metrics = c("binary_accuracy"))

```

Now we fit the model:


```{r Run Neural Net, echo=TRUE}

x_train <- as.matrix(x_train)
x_train_label <- as.matrix(x_train_label)
network %>% fit(x_train,x_train_label, epochs = 10)#, batch_size=10)

```

```{r}
x_test <- as.matrix(x_test)
x_test_label <- as.matrix(x_test_label)
metrics <- network %>% evaluate(x_test,x_test_label)
metrics
```

### Random Forest Model
```{r Random Forest Basic}
set.seed(1234)
ind <- createDataPartition(data$state, p =.8, list = FALSE)
train <- data[ind,-c(1,2,3)]
test <- data[-ind,-c(1,2,3)]
train <- na.omit(train)
test <- na.omit(test)
str(train)
randomForest_model <- randomForest(train[,-7],
                                   train$state, 
                                   sampsize = round(0.6*(length(train))), ## the number of sample size
                                   ntree = 2000, ## How many trees to use
                                   mtry = sqrt(9),
                                   importance = TRUE) ## Good for feature selection
tables <- CrossTable(train$state, randomForest_model$predicted,
                     prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
                     dnn = c('actual survived', 'predicted survived'))
tables$prop.tbl[1] + tables$prop.tbl[4]
varImpPlot(randomForest_model) 
```
From the variable importance plot it appears as though we should remove currency and country from the model
```{r Random Forest Tuned}
str(train)
randomForest_model <- randomForest(train[,-c(2,7,9)],
                                   train$state, 
                                   sampsize = round(0.6*(length(train))), ## the number of sample size
                                   ntree = 2000, ## How many trees to use
                                   mtry = sqrt(9),
                                   importance = TRUE) ## Good for feature selection
tables <- CrossTable(train$state, randomForest_model$predicted,
                     prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
                     dnn = c('actual survived', 'predicted survived'))
tables$prop.tbl[1] + tables$prop.tbl[4]
varImpPlot(randomForest_model) 
```
With this model we get an accuracy of about .85, which isn't too bad, lets see how it performs on the test data

```{r}
pred <- predict(randomForest_model, test)
tables <- CrossTable(test$state, pred,
                     prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
                     dnn = c('actual survived', 'predicted survived'))
tables$prop.tbl[1] + tables$prop.tbl[4]
```
Also about 85% accuracy which is fairly good, however lets see if we can get a better accuracy by tuning our hyper parameters

```{r Tuning RF, eval=FALSE}
cv_opts <- trainControl(method="cv", number=5)
Grid <- expand.grid(mtry = seq(1,10)) 
results_rf <- train(train[,-c(2,7,9)], train$state  ,data=train, method="rf", 
                    trControl=cv_opts, # Cv opts is for tuning hyperparameteres
                    tuneGrid=Grid)
pred <- predict(results_rf, test)
tables <- CrossTable(test$state, pred,
                     prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
                     dnn = c('actual successful', 'predicted sucessfull'))
tables$prop.tbl[1] + tables$prop.tbl[4]

```
From the tuning it was found that the best results was with an mtry of 6, and when we run that model on the test data we get an accuracy of about .995825. Which is really good. However the computational time it took to make this model was extensive. Let's see if we can get similar performance on an adaptive boosting model, which could cut down on the computational efforts

### Extreme Gradient Boosting Model
```{r Extreme Gradient Boosting, message=FALSE, warning=FALSE, output = FALSE}
response <- data[,10]
full_matrix <- sparse.model.matrix(~., data = data[,-c(1,2,3,6,8,10)])
sample <- sample.int(n = nrow(full_matrix), size = floor(.8*nrow(full_matrix)))
training <- full_matrix[sample,]
testing <- full_matrix[-sample,]

response_train <- response[sample]
response_test <- response[-sample]
response_train <- as.factor(response_train)
response_test <- as.factor(response_test)
response_train <- ifelse(response_train == 'successful', 0,1)
response_test <- ifelse(response_test == 'successful', 0,1)

dtrain <- xgb.DMatrix(data = training, label = response_train)
dtest <- xgb.DMatrix(data = testing, label =response_test)
watchlist <- list(train = dtrain, test = dtest)

params <- list(booster = "gbtree", objective = "binary:logistic", eta=.1, gamma=1, max_depth=6, min_child_weight=1, subsample=.7, colsample_bytree=1)

# xgb.fit <- xgb.cv(dtrain, nrounds = 1000, nfold = 5, params = params, verbose = 0, early_stopping_round = 10)
# xgb.fit$evaluation_log[which(xgb.fit$evaluation_log$test_error_mean == min(xgb.fit$evaluation_log$test_error_mean)),]

xgb1 <- xgb.train(params = params, data = dtrain, nrounds = 247,
                   watchlist = watchlist, print_every_n = 1, early_stop_round = 5, maximize = F , eval_metric = "error")
```

```{r Predictions}
xgbpred <- predict(xgb1,dtest)
xgbpred <- ifelse(xgbpred < 0.5,1,0)
table <- CrossTable(xgbpred, response_test,
                    prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
                    dnn = c('actual survival', 'predicted survival'))
table$prop.tbl[1] + table$prop.tbl[4]
```
